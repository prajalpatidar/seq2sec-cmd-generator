# Training configuration for seq2seq model
# Optimized for embedded systems deployment

# Model hyperparameters
embedding_dim: 256       # Increased for better representation
hidden_dim: 512          # Increased capacity for better learning
num_layers: 2            # Two layers for deeper learning

# Training hyperparameters
batch_size: 16           # Increased batch size with larger dataset
num_epochs: 250          # More epochs for convergence
learning_rate: 0.0002    # Slightly lower for stability with larger model
teacher_forcing_ratio: 0.3  # Moderate teacher forcing for balance

# Data parameters
max_vocab_size: 1500     # Increased vocabulary for expanded dataset
train_split: 0.85        # More training data

# Optimization
weight_decay: 0.0001
gradient_clip: 1.0
